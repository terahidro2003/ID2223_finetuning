{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terahidro2003/ID2223_finetuning/blob/main/ID2223_fine_tuning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "## Install Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4PAanGwH0peV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.2.1\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "  from google.colab import userdata\n",
        "  TOKEN = userdata.get('HF_TOKEN')\n",
        "else:\n",
        "  !pip install python-dotenv\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "  TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "hub_repo = \"hellstone1918/test-checkpoint-model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Quantizied Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: unsloth in /home/hellstone/.local/lib/python3.10/site-packages (2025.11.4)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.11.4 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (2025.11.5)\n",
            "Requirement already satisfied: diffusers in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.35.2)\n",
            "Requirement already satisfied: tyro in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.9.35)\n",
            "Requirement already satisfied: torch>=2.4.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (2.9.0)\n",
            "Requirement already satisfied: psutil in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (7.1.3)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.18.0)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (4.57.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (4.3.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.36.0)\n",
            "Requirement already satisfied: trl!=0.19.0,<=0.24.0,>=0.18.2 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.48.2)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.0.33.post1)\n",
            "Requirement already satisfied: torchvision in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: triton>=3.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (3.5.0)\n",
            "Requirement already satisfied: protobuf in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (6.33.1)\n",
            "Requirement already satisfied: hf_transfer in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: numpy in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (2.2.6)\n",
            "Requirement already satisfied: packaging in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (25.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/hellstone/.local/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.34.1->unsloth) (5.4.1)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.5)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.0)\n",
            "Requirement already satisfied: filelock in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
            "Requirement already satisfied: pandas in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.3.3)\n",
            "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hellstone/.local/lib/python3.10/site-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/hellstone/.local/lib/python3.10/site-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.3.3.83)\n",
            "Requirement already satisfied: jinja2 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.13.1.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/hellstone/.local/lib/python3.10/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/hellstone/.local/lib/python3.10/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
            "Requirement already satisfied: torchao>=0.13.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (0.14.1)\n",
            "Requirement already satisfied: cut_cross_entropy in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (25.1.1)\n",
            "Requirement already satisfied: msgspec in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (0.20.0)\n",
            "Requirement already satisfied: pillow in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (12.0.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (1.8.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (14.2.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hellstone/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
            "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.3)\n",
            "Requirement already satisfied: httpcore==1.* in /home/hellstone/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2020.6.20)\n",
            "Requirement already satisfied: anyio in /home/hellstone/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
            "Requirement already satisfied: h11>=0.16 in /home/hellstone/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/hellstone/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.26.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hellstone/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hellstone/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=2.4.0->unsloth) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hellstone/.local/lib/python3.10/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/hellstone/.local/lib/python3.10/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2022.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (5.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/hellstone/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/hellstone/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/hellstone/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
            "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070. Num GPUs = 1. Max memory: 11.994 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Dataset Format to HuggingFace's Generic Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n",
              "  'role': 'user'},\n",
              " {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "import multiprocessing\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir       = \"outputs\",\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps     = 5,\n",
        "    max_steps        = 100,\n",
        "    learning_rate    = 2e-4,\n",
        "    fp16             = not is_bfloat16_supported(),\n",
        "    bf16             = is_bfloat16_supported(),\n",
        "    logging_steps    = 1,\n",
        "    optim            = \"adamw_8bit\",\n",
        "    weight_decay     = 0.01,\n",
        "    lr_scheduler_type= \"linear\",\n",
        "    seed             = 3407,\n",
        "\n",
        "    # Checkpoint saving and pushing config\n",
        "    push_to_hub = True,\n",
        "    hub_model_id = hub_repo,\n",
        "    hub_token=TOKEN,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps       = 10,\n",
        "    save_total_limit = 5,\n",
        "    hub_strategy=\"all_checkpoints\",\n",
        "\n",
        "    report_to        = \"none\",\n",
        "\n",
        "    dataset_num_proc = multiprocessing.cpu_count(),\n",
        "    packing          = False,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model              = model,\n",
        "    tokenizer          = tokenizer,\n",
        "    train_dataset      = dataset,\n",
        "    args               = sft_config,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length     = max_seq_length,\n",
        "    data_collator      = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from huggingface_hub import HfApi, snapshot_download\n",
        "\n",
        "def get_latest_hf_checkpoint(repo_id: str, cache_dir: str | None = None) -> str | None:\n",
        "    \"\"\"\n",
        "    Returns local path to the latest checkpoint-* directory from a Hub repo,\n",
        "    or None if the repo doesn't exist or has no checkpoints.\n",
        "    \"\"\"\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        api.repo_info(repo_id, repo_type=\"model\")\n",
        "    except:\n",
        "        # Repo does not exist on the Hub -> first training ever\n",
        "        return None\n",
        "\n",
        "    # Repo exists – download files locally (or reuse cached snapshot)\n",
        "    local_repo_path = snapshot_download(\n",
        "        repo_id,\n",
        "        repo_type=\"model\",\n",
        "        local_dir=cache_dir,                # e.g. \"hf_repo_cache\" or None\n",
        "        local_dir_use_symlinks=False,\n",
        "    )\n",
        "\n",
        "    # Find checkpoint-* subdirectories\n",
        "    ckpts: list[tuple[int, str]] = []\n",
        "    for name in os.listdir(local_repo_path):\n",
        "        full = os.path.join(local_repo_path, name)\n",
        "        m = re.match(r\"checkpoint-(\\d+)\", name)\n",
        "        if m and os.path.isdir(full):\n",
        "            step = int(m.group(1))\n",
        "            ckpts.append((step, full))\n",
        "\n",
        "    if not ckpts:\n",
        "        # Repo exists but no checkpoints yet (maybe only final model)\n",
        "        return None\n",
        "\n",
        "    # Return path of checkpoint with highest step number\n",
        "    _, latest_ckpt_path = max(ckpts, key=lambda t: t[0])\n",
        "    return latest_ckpt_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found checkpoint on Hub: /home/hellstone/.cache/huggingface/hub/models--hellstone1918--test-checkpoint-model/snapshots/db1a2e8da5b43ee48962b8421a8466ba74cc18bd/checkpoint-60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 02:30, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.619200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.766000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.699300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.905200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.686800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.797900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.717700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.785200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.522000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.395400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.604200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.783400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.806000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.595400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.853900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.891800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.706900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.561100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.716500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.711600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.909500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.814400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.555700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.798900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.547200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.718700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.086100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.693300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.616700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.461800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.552700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.606000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.656400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.637600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")\n",
        "latest_ckpt = get_latest_hf_checkpoint(hub_repo)\n",
        "if latest_ckpt is None:\n",
        "    print(\"No checkpoints on Hub – starting training from scratch.\")\n",
        "    trainer_stats = trainer.train()\n",
        "else:\n",
        "    print(f\"Found checkpoint on Hub: {latest_ckpt}\")\n",
        "    trainer_stats = trainer.train(resume_from_checkpoint=latest_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'start_gpu_memory' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[1;32m      2\u001b[0m used_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m used_memory_for_lora \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory \u001b[38;5;241m-\u001b[39m \u001b[43mstart_gpu_memory\u001b[49m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m used_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory         \u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m lora_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(used_memory_for_lora\u001b[38;5;241m/\u001b[39mmax_memory\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers. The sequence is: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, and so on.\n",
            "####\n",
            "2, 3, 5, 8<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting model to GGUF format...\n",
            "Unsloth: Merging model weights to 16-bit format...\n",
            "Found HuggingFace hub cache directory: /home/hellstone/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|████████████████████████████████| 2/2 [01:04<00:00, 32.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging weights into 16bit: 100%|██████████████████████████████████████| 2/2 [00:24<00:00, 12.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merge process complete. Saved to `/tmp/unsloth_gguf_eotv3lg4`\n",
            "Unsloth: Converting to GGUF format...\n",
            "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF bf16 to ['f16'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: llama.cpp folder exists but binaries not found - will rebuild\n",
            "Unsloth: Updating system package directories\n",
            "Unsloth: Missing packages: libcurl4-openssl-dev\n",
            "Unsloth: Will attempt to install missing system packages.\n",
            "Unsloth: Installing packages: libcurl4-openssl-dev\n",
            "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
            "Unsloth: Install GGUF and other packages\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to convert model to GGUF: Unsloth: GGUF conversion failed: === Unsloth: FAILED building llama.cpp ===\nMake failed: [FAIL] Command `make clean` failed with exit code 2\nstdout: Makefile:6: *** Build system changed:\n The Makefile build has been replaced by CMake.\n\n For build instructions see:\n https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n\n.  Stop.\n\n\nCMake failed: [FAIL] Command `cmake . -B build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON` failed with exit code 1\nstdout: -- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\") \n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE  \n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n-- Found OpenMP: TRUE (found version \"4.5\")  \n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- ggml version: 0.9.4\n-- ggml commit:  583cb8341\n-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR) \n\u001b[31mCMake Error at common/CMakeLists.txt:91 (message):\n  Could NOT find CURL.  Hint: to disable this feature, set -DLLAMA_CURL=OFF\n\n\u001b[0m\n-- Configuring incomplete, errors occurred!\nSee also \"/mnt/f/University/KTH/ml_finetuning/ID2223_finetuning/llama.cpp/build/CMakeFiles/CMakeOutput.log\".\n\n\n=== Full output log: ===\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: gguf in /home/hellstone/.local/lib/python3.10/site-packages (0.17.1)\nRequirement already satisfied: protobuf in /home/hellstone/.local/lib/python3.10/site-packages (6.33.1)\nRequirement already satisfied: sentencepiece in /home/hellstone/.local/lib/python3.10/site-packages (0.2.1)\nRequirement already satisfied: mistral_common in /home/hellstone/.local/lib/python3.10/site-packages (1.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from gguf) (5.4.1)\nRequirement already satisfied: tqdm>=4.27 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (4.67.1)\nRequirement already satisfied: numpy>=1.17 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (2.2.6)\nRequirement already satisfied: pillow>=10.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (12.0.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (0.12.0)\nRequirement already satisfied: typing-extensions>=4.11.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.15.0)\nRequirement already satisfied: pydantic<3.0,>=2.7 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.12.4)\nRequirement already satisfied: requests>=2.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.32.5)\nRequirement already satisfied: jsonschema>=4.21.1 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.25.1)\nRequirement already satisfied: pydantic-extra-types[pycountry]>=2.10.5 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.10.6)\nRequirement already satisfied: referencing>=0.28.4 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.37.0)\nRequirement already satisfied: attrs>=22.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (25.4.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.29.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (2025.9.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.2)\nRequirement already satisfied: pycountry>=23 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (24.6.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (1.26.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/hellstone/.local/lib/python3.10/site-packages (from requests>=2.0.0->mistral_common) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (2020.6.20)\nRequirement already satisfied: regex>=2022.1.18 in /home/hellstone/.local/lib/python3.10/site-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth/save.py:1191\u001b[0m, in \u001b[0;36msave_to_gguf\u001b[0;34m(model_name, model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, is_vlm, is_gpt_oss)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1191\u001b[0m     quantizer_location, converter_location \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_llama_cpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: llama.cpp found in the system. Skipping installation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth_zoo/llama_cpp.py:343\u001b[0m, in \u001b[0;36mcheck_llama_cpp\u001b[0;34m(llama_cpp_folder)\u001b[0m\n\u001b[1;32m    342\u001b[0m     files_found \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(llama_cpp_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: No working quantizer found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles in directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(f)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mf\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mfiles_found[:\u001b[38;5;241m20\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: No working quantizer found in llama.cpp\nFiles in directory: AUTHORS, benches, build, build-xcframework.sh, ci, cmake, CMakeLists.txt, CMakePresets.json, CODEOWNERS, common, CONTRIBUTING.md, convert_hf_to_gguf.py, convert_hf_to_gguf_update.py, convert_llama_ggml_to_gguf.py, convert_lora_to_gguf.py, docs, examples, flake.lock, flake.nix, ggml",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth/save.py:1966\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m     all_file_locations, want_full_precision, is_vlm_update \u001b[38;5;241m=\u001b[39m \u001b[43msave_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_sentencepiece\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquantization_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass VLM flag\u001b[39;49;00m\n\u001b[1;32m   1975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_gpt_oss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_gpt_oss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass gpt_oss Flag\u001b[39;49;00m\n\u001b[1;32m   1976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth/save.py:1201\u001b[0m, in \u001b[0;36msave_to_gguf\u001b[0;34m(model_name, model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, is_vlm, is_gpt_oss)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m         quantizer_location, converter_location \u001b[38;5;241m=\u001b[39m \u001b[43minstall_llama_cpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgpu_support\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GGUF conversion doesn't need CUDA\u001b[39;49;00m\n\u001b[1;32m   1203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprint_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprint_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# Step 2: Download and patch converter script\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth_zoo/llama_cpp.py:467\u001b[0m, in \u001b[0;36minstall_llama_cpp\u001b[0;34m(llama_cpp_folder, llama_cpp_targets, print_output, gpu_support, just_clone_repo)\u001b[0m\n\u001b[1;32m    466\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(print_outputs)\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Check if it installed correctly\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: === Unsloth: FAILED building llama.cpp ===\nMake failed: [FAIL] Command `make clean` failed with exit code 2\nstdout: Makefile:6: *** Build system changed:\n The Makefile build has been replaced by CMake.\n\n For build instructions see:\n https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n\n.  Stop.\n\n\nCMake failed: [FAIL] Command `cmake . -B build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON` failed with exit code 1\nstdout: -- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\") \n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE  \n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n-- Found OpenMP: TRUE (found version \"4.5\")  \n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- ggml version: 0.9.4\n-- ggml commit:  583cb8341\n-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR) \n\u001b[31mCMake Error at common/CMakeLists.txt:91 (message):\n  Could NOT find CURL.  Hint: to disable this feature, set -DLLAMA_CURL=OFF\n\n\u001b[0m\n-- Configuring incomplete, errors occurred!\nSee also \"/mnt/f/University/KTH/ml_finetuning/ID2223_finetuning/llama.cpp/build/CMakeFiles/CMakeOutput.log\".\n\n\n=== Full output log: ===\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: gguf in /home/hellstone/.local/lib/python3.10/site-packages (0.17.1)\nRequirement already satisfied: protobuf in /home/hellstone/.local/lib/python3.10/site-packages (6.33.1)\nRequirement already satisfied: sentencepiece in /home/hellstone/.local/lib/python3.10/site-packages (0.2.1)\nRequirement already satisfied: mistral_common in /home/hellstone/.local/lib/python3.10/site-packages (1.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from gguf) (5.4.1)\nRequirement already satisfied: tqdm>=4.27 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (4.67.1)\nRequirement already satisfied: numpy>=1.17 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (2.2.6)\nRequirement already satisfied: pillow>=10.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (12.0.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (0.12.0)\nRequirement already satisfied: typing-extensions>=4.11.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.15.0)\nRequirement already satisfied: pydantic<3.0,>=2.7 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.12.4)\nRequirement already satisfied: requests>=2.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.32.5)\nRequirement already satisfied: jsonschema>=4.21.1 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.25.1)\nRequirement already satisfied: pydantic-extra-types[pycountry]>=2.10.5 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.10.6)\nRequirement already satisfied: referencing>=0.28.4 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.37.0)\nRequirement already satisfied: attrs>=22.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (25.4.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.29.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (2025.9.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.2)\nRequirement already satisfied: pycountry>=23 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (24.6.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (1.26.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/hellstone/.local/lib/python3.10/site-packages (from requests>=2.0.0->mistral_common) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (2020.6.20)\nRequirement already satisfied: regex>=2022.1.18 in /home/hellstone/.local/lib/python3.10/site-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth/save.py:2115\u001b[0m, in \u001b[0;36munsloth_push_to_hub_gguf\u001b[0;34m(self, repo_id, tokenizer, quantization_method, first_conversion, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2114\u001b[0m     \u001b[38;5;66;03m# Call save_pretrained_gguf - it returns all the info we need\u001b[39;00m\n\u001b[0;32m-> 2115\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_save_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Never push from here\u001b[39;49;00m\n\u001b[1;32m   2122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't need token for local save\u001b[39;49;00m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemporary_location\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemporary_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_memory_usage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmaximum_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2129\u001b[0m     \u001b[38;5;66;03m# Extract results\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth/save.py:1986\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1986\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: GGUF conversion failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# Step 9: Create Ollama modelfile\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: GGUF conversion failed: === Unsloth: FAILED building llama.cpp ===\nMake failed: [FAIL] Command `make clean` failed with exit code 2\nstdout: Makefile:6: *** Build system changed:\n The Makefile build has been replaced by CMake.\n\n For build instructions see:\n https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n\n.  Stop.\n\n\nCMake failed: [FAIL] Command `cmake . -B build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON` failed with exit code 1\nstdout: -- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\") \n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE  \n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n-- Found OpenMP: TRUE (found version \"4.5\")  \n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- ggml version: 0.9.4\n-- ggml commit:  583cb8341\n-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR) \n\u001b[31mCMake Error at common/CMakeLists.txt:91 (message):\n  Could NOT find CURL.  Hint: to disable this feature, set -DLLAMA_CURL=OFF\n\n\u001b[0m\n-- Configuring incomplete, errors occurred!\nSee also \"/mnt/f/University/KTH/ml_finetuning/ID2223_finetuning/llama.cpp/build/CMakeFiles/CMakeOutput.log\".\n\n\n=== Full output log: ===\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: gguf in /home/hellstone/.local/lib/python3.10/site-packages (0.17.1)\nRequirement already satisfied: protobuf in /home/hellstone/.local/lib/python3.10/site-packages (6.33.1)\nRequirement already satisfied: sentencepiece in /home/hellstone/.local/lib/python3.10/site-packages (0.2.1)\nRequirement already satisfied: mistral_common in /home/hellstone/.local/lib/python3.10/site-packages (1.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from gguf) (5.4.1)\nRequirement already satisfied: tqdm>=4.27 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (4.67.1)\nRequirement already satisfied: numpy>=1.17 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (2.2.6)\nRequirement already satisfied: pillow>=10.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (12.0.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (0.12.0)\nRequirement already satisfied: typing-extensions>=4.11.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.15.0)\nRequirement already satisfied: pydantic<3.0,>=2.7 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.12.4)\nRequirement already satisfied: requests>=2.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.32.5)\nRequirement already satisfied: jsonschema>=4.21.1 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.25.1)\nRequirement already satisfied: pydantic-extra-types[pycountry]>=2.10.5 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.10.6)\nRequirement already satisfied: referencing>=0.28.4 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.37.0)\nRequirement already satisfied: attrs>=22.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (25.4.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.29.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (2025.9.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.2)\nRequirement already satisfied: pycountry>=23 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (24.6.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (1.26.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/hellstone/.local/lib/python3.10/site-packages (from requests>=2.0.0->mistral_common) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (2020.6.20)\nRequirement already satisfied: regex>=2022.1.18 in /home/hellstone/.local/lib/python3.10/site-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhub_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/unsloth/save.py:2145\u001b[0m, in \u001b[0;36munsloth_push_to_hub_gguf\u001b[0;34m(self, repo_id, tokenizer, quantization_method, first_conversion, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2143\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   2144\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 2145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert model to GGUF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;66;03m# Step 3: Upload to HuggingFace Hub\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Uploading GGUF to Huggingface Hub...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to convert model to GGUF: Unsloth: GGUF conversion failed: === Unsloth: FAILED building llama.cpp ===\nMake failed: [FAIL] Command `make clean` failed with exit code 2\nstdout: Makefile:6: *** Build system changed:\n The Makefile build has been replaced by CMake.\n\n For build instructions see:\n https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n\n.  Stop.\n\n\nCMake failed: [FAIL] Command `cmake . -B build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON` failed with exit code 1\nstdout: -- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\") \n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE  \n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n-- Found OpenMP: TRUE (found version \"4.5\")  \n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- ggml version: 0.9.4\n-- ggml commit:  583cb8341\n-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR) \n\u001b[31mCMake Error at common/CMakeLists.txt:91 (message):\n  Could NOT find CURL.  Hint: to disable this feature, set -DLLAMA_CURL=OFF\n\n\u001b[0m\n-- Configuring incomplete, errors occurred!\nSee also \"/mnt/f/University/KTH/ml_finetuning/ID2223_finetuning/llama.cpp/build/CMakeFiles/CMakeOutput.log\".\n\n\n=== Full output log: ===\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: gguf in /home/hellstone/.local/lib/python3.10/site-packages (0.17.1)\nRequirement already satisfied: protobuf in /home/hellstone/.local/lib/python3.10/site-packages (6.33.1)\nRequirement already satisfied: sentencepiece in /home/hellstone/.local/lib/python3.10/site-packages (0.2.1)\nRequirement already satisfied: mistral_common in /home/hellstone/.local/lib/python3.10/site-packages (1.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from gguf) (5.4.1)\nRequirement already satisfied: tqdm>=4.27 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (4.67.1)\nRequirement already satisfied: numpy>=1.17 in /home/hellstone/.local/lib/python3.10/site-packages (from gguf) (2.2.6)\nRequirement already satisfied: pillow>=10.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (12.0.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (0.12.0)\nRequirement already satisfied: typing-extensions>=4.11.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.15.0)\nRequirement already satisfied: pydantic<3.0,>=2.7 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.12.4)\nRequirement already satisfied: requests>=2.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.32.5)\nRequirement already satisfied: jsonschema>=4.21.1 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (4.25.1)\nRequirement already satisfied: pydantic-extra-types[pycountry]>=2.10.5 in /home/hellstone/.local/lib/python3.10/site-packages (from mistral_common) (2.10.6)\nRequirement already satisfied: referencing>=0.28.4 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.37.0)\nRequirement already satisfied: attrs>=22.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (25.4.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (0.29.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hellstone/.local/lib/python3.10/site-packages (from jsonschema>=4.21.1->mistral_common) (2025.9.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.2)\nRequirement already satisfied: pycountry>=23 in /home/hellstone/.local/lib/python3.10/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (24.6.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (1.26.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /home/hellstone/.local/lib/python3.10/site-packages (from requests>=2.0.0->mistral_common) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.0.0->mistral_common) (2020.6.20)\nRequirement already satisfied: regex>=2022.1.18 in /home/hellstone/.local/lib/python3.10/site-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n"
          ]
        }
      ],
      "source": [
        "model.push_to_hub_gguf(hub_repo, tokenizer, quantization_method=\"f16\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMj+DfoHkG2VBAKYqlE8S12",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
