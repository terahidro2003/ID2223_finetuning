{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terahidro2003/ID2223_finetuning/blob/main/ID2223_fine_tuning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "## Install Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4PAanGwH0peV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: python-dotenv in /home/hellstone/.local/lib/python3.10/site-packages (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "  from google.colab import userdata\n",
        "  TOKEN = userdata.get('HF_TOKEN')\n",
        "else:\n",
        "  !pip install python-dotenv\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "  TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "hub_repo = \"hellstone1918/Llama-3.2-3B-basic-lora-model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Quantizied Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting unsloth\n",
            "  Using cached unsloth-2025.11.4-py3-none-any.whl (358 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (3.5.0)\n",
            "Requirement already satisfied: tyro in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.9.35)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: torchvision in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: hf_transfer in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.48.2)\n",
            "Requirement already satisfied: torch>=2.4.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (2.9.0)\n",
            "Requirement already satisfied: protobuf in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (6.33.1)\n",
            "Requirement already satisfied: packaging in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (25.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.18.0)\n",
            "Requirement already satisfied: numpy in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (2.2.6)\n",
            "Requirement already satisfied: trl!=0.19.0,<=0.24.0,>=0.18.2 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.24.0)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.0.33.post1)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.36.0)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (4.57.1)\n",
            "Requirement already satisfied: diffusers in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (0.35.2)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.11.4 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (2025.11.5)\n",
            "Requirement already satisfied: psutil in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (7.1.3)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (1.12.0)\n",
            "Requirement already satisfied: datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (4.3.0)\n",
            "Requirement already satisfied: tqdm in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.34.1->unsloth) (5.4.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/hellstone/.local/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth) (0.7.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.5)\n",
            "Requirement already satisfied: xxhash in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\n",
            "Requirement already satisfied: pandas in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.3.3)\n",
            "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.9.0)\n",
            "Requirement already satisfied: filelock in /home/hellstone/.local/lib/python3.10/site-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hellstone/.local/lib/python3.10/site-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/hellstone/.local/lib/python3.10/site-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.4.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.13.1.3)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.5.8.93)\n",
            "Requirement already satisfied: jinja2 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/hellstone/.local/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.7.3.90)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/hellstone/.local/lib/python3.10/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/hellstone/.local/lib/python3.10/site-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth) (0.22.1)\n",
            "Requirement already satisfied: msgspec in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (0.20.0)\n",
            "Requirement already satisfied: pillow in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (12.0.0)\n",
            "Requirement already satisfied: torchao>=0.13.0 in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (0.14.1)\n",
            "Requirement already satisfied: cut_cross_entropy in /home/hellstone/.local/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.4->unsloth) (25.1.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (1.8.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /home/hellstone/.local/lib/python3.10/site-packages (from tyro->unsloth) (14.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hellstone/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2020.6.20)\n",
            "Requirement already satisfied: anyio in /home/hellstone/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.11.0)\n",
            "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.3)\n",
            "Requirement already satisfied: httpcore==1.* in /home/hellstone/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/hellstone/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.26.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/hellstone/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hellstone/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hellstone/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=2.4.0->unsloth) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hellstone/.local/lib/python3.10/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/hellstone/.local/lib/python3.10/site-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (5.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/hellstone/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/hellstone/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/hellstone/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/hellstone/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.3.1)\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2025.11.4\n",
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hellstone/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070. Num GPUs = 1. Max memory: 11.994 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hellstone/.local/lib/python3.10/site-packages/peft/tuners/lora/config.py:730: UserWarning: `loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\n",
            "  warnings.warn(\"`loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\")\n",
            "Unsloth 2025.11.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # 0 is optimized by unsloth (fallback, if needed: 0.05)\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = True, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Dataset Format to HuggingFace's Generic Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n",
              "  'role': 'user'},\n",
              " {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "import multiprocessing\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir       = \"outputs\",\n",
        "    # batch size - number of training examples utilized in one iteration\n",
        "    # epoch - one forward and backward pass of all training samples\n",
        "    # epoch = iterations * batch_size\n",
        "    # effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "    per_device_train_batch_size = 2, # determines how many samples are loaded by the GPU at once (consumes more VRAM, but learning gets faster)\n",
        "    gradient_accumulation_steps = 16,\n",
        "\n",
        "    # warmup is needed at the beginning of the training as weights are unadapted to the new dataset. Therefore, we need to slowly increase the learning rate as we fine-tune the model.\n",
        "    # - prevents large initial updates that could destabilize the model\n",
        "    # - allows the model to explore the parameter space more effectively and transition smoothly from initial parameter space to regions of higher gradients\n",
        "    # for large datasets or large amount of features, 5-10% of warmup steps is recommended. Since we use reletively small dataset (100k), it might be safe to go with 3-5% warmup steps (from overall steps)\n",
        "    warmup_steps     = 150, # how many steps you use to slowly increase the learning rate from 0 up to the target learning rate\n",
        "\n",
        "    # too many steps -> overfitting or model collapse, too few steps -> catastrophic forgetting\n",
        "    max_steps        = 5000,\n",
        "\n",
        "    learning_rate    = 2e-4,\n",
        "\n",
        "    fp16             = not is_bfloat16_supported(),\n",
        "    bf16             = is_bfloat16_supported(),\n",
        "    logging_steps    = 1,\n",
        "    optim            = \"adamw_8bit\",\n",
        "    weight_decay     = 0.01, # adds a small penalty to large weights during training.\n",
        "    lr_scheduler_type= \"linear\",\n",
        "    seed             = 3407,\n",
        "\n",
        "    # Checkpoint saving and pushing config\n",
        "    push_to_hub = True,\n",
        "    hub_model_id = hub_repo,\n",
        "    hub_token=TOKEN,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps       = 250,\n",
        "    save_total_limit = 5,\n",
        "    hub_strategy=\"all_checkpoints\",\n",
        "\n",
        "    report_to        = \"none\",\n",
        "\n",
        "    dataset_num_proc = multiprocessing.cpu_count(),\n",
        "    packing          = False,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model              = model,\n",
        "    tokenizer          = tokenizer,\n",
        "    train_dataset      = dataset,\n",
        "    args               = sft_config,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length     = max_seq_length,\n",
        "    data_collator      = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from huggingface_hub import HfApi, snapshot_download\n",
        "\n",
        "def get_latest_hf_checkpoint(repo_id: str, cache_dir: str | None = None) -> str | None:\n",
        "    \"\"\"\n",
        "    Returns local path to the latest checkpoint-* directory from a Hub repo,\n",
        "    or None if the repo doesn't exist or has no checkpoints.\n",
        "    \"\"\"\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        api.repo_info(repo_id, repo_type=\"model\")\n",
        "    except:\n",
        "        # Repo does not exist on the Hub -> first training ever\n",
        "        return None\n",
        "\n",
        "    # Repo exists â€“ download files locally (or reuse cached snapshot)\n",
        "    local_repo_path = snapshot_download(\n",
        "        repo_id,\n",
        "        repo_type=\"model\",\n",
        "        local_dir=cache_dir,                # e.g. \"hf_repo_cache\" or None\n",
        "        local_dir_use_symlinks=False,\n",
        "    )\n",
        "\n",
        "    # Find checkpoint-* subdirectories\n",
        "    ckpts: list[tuple[int, str]] = []\n",
        "    for name in os.listdir(local_repo_path):\n",
        "        full = os.path.join(local_repo_path, name)\n",
        "        m = re.match(r\"checkpoint-(\\d+)\", name)\n",
        "        if m and os.path.isdir(full):\n",
        "            step = int(m.group(1))\n",
        "            ckpts.append((step, full))\n",
        "\n",
        "    if not ckpts:\n",
        "        # Repo exists but no checkpoints yet (maybe only final model)\n",
        "        return None\n",
        "\n",
        "    # Return path of checkpoint with highest step number\n",
        "    _, latest_ckpt_path = max(ckpts, key=lambda t: t[0])\n",
        "    return latest_ckpt_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 4070. Max memory = 11.994 GB.\n",
            "3.07 GB of memory reserved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found checkpoint on Hub: /home/hellstone/.cache/huggingface/hub/models--hellstone1918--Llama-3.2-3B-basic-lora-model/snapshots/83ccb953608d3c1dd39b21ed026367f4f14d8940/checkpoint-5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100,000 | Num Epochs = 2 | Total steps = 5,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 16\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 16 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 48,627,712 of 3,261,377,536 (1.49% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5001' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 00:07, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5001</td>\n",
              "      <td>0.601800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")\n",
        "latest_ckpt = get_latest_hf_checkpoint(hub_repo)\n",
        "if latest_ckpt is None:\n",
        "    print(\"No checkpoints on Hub â€“ starting training from scratch.\")\n",
        "    trainer_stats = trainer.train()\n",
        "else:\n",
        "    print(f\"Found checkpoint on Hub: {latest_ckpt}\")\n",
        "    trainer_stats = trainer.train(resume_from_checkpoint=latest_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31.8118 seconds used for training.\n",
            "0.53 minutes used for training.\n",
            "Peak reserved memory = 4.863 GB.\n",
            "Peak reserved memory for training = 1.793 GB.\n",
            "Peak reserved memory % of max memory = 40.545 %.\n",
            "Peak reserved memory for training % of max memory = 14.949 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JVM (Java Virtual Machine) is a software layer that runs on top of the CPU's hardware. It is a component of Java development platform which is responsible to handle the execution of Java programs. It provides the following features: memory management, execution, runtime data access. JVM is designed to work with any environment in which Java programs can run, allowing them to be executed anywhere there is an JVM installed. It provides the runtime environment in which Java programs can run.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is JVM?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 256,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPython \u001b[36m3.11.14\u001b[39m\u001b[36m\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[33m?\u001b[0m \u001b[1mA virtual environment already exists at `.venv`. Do you want to replace it?\u001b[0m \u001b[38;5;8m[y/n]\u001b[0m \u001b[38;5;8mâ€º\u001b[0m \u001b[36myes\u001b[0m\n",
            "\n",
            "\u001b[36m\u001b[1mhint\u001b[0m\u001b[1m:\u001b[0m Use the `\u001b[32m--clear\u001b[39m` flag or set `\u001b[32mUV_VENV_CLEAR=1\u001b[39m` to skip this prompt\u001b[?25l\u001b[?25h\n",
            "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 1.70s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Files (1 / 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  195MB /  195MB, 79.7MB/s  \n",
            "New Data Upload: |                                                                                                                                                                  |  0.00B /  0.00B,  0.00B/s  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/hellstone1918/Llama-3.2-3B-basic-lora-model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Files (1 / 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB,  0.00B/s  \n",
            "New Data Upload: |                                                                                                                                                                  |  0.00B /  0.00B,  0.00B/s  \n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "[huggingface_hub.hf_api|WARNING]No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "!uv venv .venv\n",
        "!source .venv/bin/activate --clear\n",
        "!uv pip install \"unsloth\" \"gguf\" \"protobuf\" \"sentencepiece\" \"mistral_common\"\n",
        "\n",
        "model.save_pretrained(MODEL_NAME + \"_regular_lora\")\n",
        "tokenizer.save_pretrained(MODEL_NAME + \"_regular_lora\")\n",
        "\n",
        "model.push_to_hub_gguf(hub_repo, tokenizer, quantization_method = \"q4_k_m\")\n",
        "\n",
        "model.push_to_hub(hub_repo)\n",
        "tokenizer.push_to_hub(hub_repo)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMj+DfoHkG2VBAKYqlE8S12",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
