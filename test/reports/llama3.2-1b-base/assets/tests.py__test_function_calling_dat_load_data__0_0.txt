Here's an example of how you can load data from an S3 bucket to an analytics database in Parquet format using AWS services such as S3, S3FS, and Amazon Redshift.

**Step 1: Create an S3FS connection**

First, you need to create an S3FS connection to access your S3 bucket. You can do this by creating a new IAM role with the necessary permissions, and then attaching the role to an EC2 instance.

```bash
aws s3fs attach --bucket <bucket-name> --key <object-key> --role <access-key> --profile <profile-name>
```

Replace `<bucket-name>`, `<object-key>`, and `<access-key>` with your actual S3 bucket name, object key, and access key.

**Step 2: Create an Amazon Redshift cluster**

Next, you need to create an Amazon Redshift cluster to store your data. You can do this by creating a new Redshift cluster using the AWS Management Console.

```bash
aws redshift create-cluster --cluster-name <cluster-name> --database-name <database-name> --instance-type <instance-type> --num-workers <num-workers> --security-group <security-group> --vpc-config <vpc-config> --endpoint-type <endpoint-type>
```

Replace `<cluster-name>`, `<database-name>`, `<instance-type>`, `<num-workers>`, `<security-group>`, and `<vpc-config>` with your actual Redshift cluster name, database name, instance type, number of workers, security group, and VPC configuration.

**Step 3: Create a table in Redshift**

Now, you need to create a table in Redshift to store your data. You can do this by creating a new table using the AWS Management Console.

```bash
aws redshift create-table --table-name <table-name> --database-name <database-name> --schema <schema> --partition-key <partition-key> --columnDefinitions <column-definition> --rowDefinitions <row-definition>
```

Replace `<table-name>`, `<database-name>`, `<schema>`, `<partition-key>`, and `<column-definition>` with your actual Redshift table name, database name, schema, partition key, and column definition.

**Step 4: Load data from S3 to Redshift**

Now, you can load data from S3 to Redshift using the `load_data` function.

```python
import boto3
import pandas as pd

# Create an S3FS connection
s3fs = boto3.client('s3fs')

# Create an Amazon Redshift cluster
redshift = boto3.client('redshift')

# Create a table in Redshift
create_table = boto3.client('redshift')
table_name = 'analytics_db.sales'
schema = {'name': 'sales', 'type': 'table'}
partition_key = 'date'
column_definitions = ['id', 'name', 'price']
row_definitions = ['id', 'name', 'price']

create_table(table_name, schema, partition_key, column_definitions, row_definitions)

# Load data from S3 to Redshift
load_data = boto3.client('redshift')
load_data(source='s3://<bucket-name>/data.csv', destination='<table-name>')

# Load data from S3 to Redshift
load_data(source='s3://<bucket-name>/data.csv', destination='<table-name>', format='csv')
```

Replace `<bucket-name>`, `<table-name>`, and `<source>` with your actual S3 bucket name, table name, and source.

**Step 5: Optimize storage**

Finally, you can optimize storage for your table by creating an index.

```python
# Create an index
create_index = boto3.client('redshift')
index_name = 'sales_index'
create_index(table_name, index_name, index_type='hash')

# Optimize storage
optimize_table = boto3.client('redshift')
optimize_table(table_name, operation='optimize')
```

Replace `<index_name>` with your actual index name.

This is a basic example of how you can load data from S3 to an analytics database in Parquet format using AWS services. You can customize this example to fit your specific use case.